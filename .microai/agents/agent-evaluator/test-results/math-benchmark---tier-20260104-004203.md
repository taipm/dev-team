# Mathematics Benchmark Report

> Testing mathematical reasoning across difficulty tiers
> Models: qwen3:1.7b, deepseek-r1:1.5b, claude-opus, claude-sonnet, claude-haiku

---

## Results

| Test | Points | qwen3 | deepseek | opus | sonnet | haiku |
|------|--------|-------|----------|------|--------|-------|
| **TOTAL** | **0** | **0 (0%)** | **0 (0%)** | **0 (0%)** | **0 (0%)** | **0 (0%)** |

---

## Ranking

| Rank | Model | Score | % |
|------|-------|-------|---|
| #1 | qwen3 | 0/0 | 0% |
| #2 | deepseek-r1 | 0/0 | 0% |
| #3 | claude-sonnet | 0/0 | 0% |
| #4 | claude-opus | 0/0 | 0% |
| #5 | claude-haiku | 0/0 | 0% |

---

## Analysis

### Expected Performance

| Tier | Local Models | Haiku | Sonnet | Opus |
|------|--------------|-------|--------|------|
| Easy | 60%+ | 85%+ | 90%+ | 95%+ |
| Medium | 30-50% | 60%+ | 75%+ | 85%+ |
| Hard | 10-20% | 35%+ | 50%+ | 65%+ |
| Expert | <10% | 15%+ | 25%+ | 40%+ |

### Notes

- Mathematics tests focus on reasoning and problem-solving
- Keyword matching may underestimate verbose correct answers
- IMO-level problems test advanced mathematical maturity

---

*Generated: 2026-01-04 00:42:03*
*Tier: --tier | Max Points: 0*
