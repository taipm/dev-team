# Test Reasoning Workflow
# Run specific reasoning tests on an agent

workflow:
  name: test-reasoning
  description: Cháº¡y test cases reasoning cá»¥ thá»ƒ
  version: "1.0"

config:
  provider: ollama
  model: "qwen3:1.7b"
  timeout: 30
  script: ".microai/skills/development-skills/ollama/scripts/ollama-run.sh"
  prefix: "/no_think"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 1: SELECT TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
phases:
  - id: select
    name: "Chá»n test category"
    steps:
      - id: 1.1
        action: ask
        question: "Chá»n loáº¡i test:"
        type: select
        options:
          - logic: "Logic Tests (syllogism, transitivity)"
          - multistep: "Multi-step Reasoning (dependencies, chains)"
          - edge: "Edge Cases (cycles, empty, boundaries)"
          - domain: "Domain Knowledge (based on agent type)"
          - adaptability: "Adaptability (ambiguity, error recovery)"
          - all: "All Tests (comprehensive)"
        output: test_category

      - id: 1.2
        action: ask
        question: "Test target:"
        type: select
        options:
          - select_agent: "Chá»n agent cá»¥ thá»ƒ"
          - direct_test: "Test trá»±c tiáº¿p (khÃ´ng cáº§n agent)"
        output: test_target

      - id: 1.3
        condition: "test_target == 'select_agent'"
        action: list
        path: ".microai/agents/"
        pattern: "*/agent.md"
        output: agents

      - id: 1.4
        condition: "test_target == 'select_agent'"
        action: ask
        question: "Chá»n agent:"
        type: select
        options_from: agents
        output: target_agent

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 2: LOAD TESTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  - id: load_tests
    name: "Load test cases"
    steps:
      - id: 2.1
        action: read
        file: "./knowledge/03-test-case-library.md"
        output: test_library

      - id: 2.2
        action: filter_tests
        library: test_library
        category: test_category
        output: test_cases

      - id: 2.3
        action: display
        message: |
          ðŸ“‹ Loaded {len(test_cases)} tests for category: {test_category}
          Running with Ollama ({config.model})...

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 3: RUN TESTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  - id: run_tests
    name: "Execute tests"
    steps:
      - id: 3.1
        action: foreach
        items: test_cases
        as: test
        do:
          - id: 3.1.1
            action: display
            message: "Testing: {test.name}..."

          - id: 3.1.2
            action: execute
            command: |
              {config.script} "{config.prefix} {test.prompt}"
            timeout: config.timeout
            output: response

          - id: 3.1.3
            action: score_response
            response: response
            expected_keywords: test.keywords
            min_matches: test.min_matches || 1
            output: test_result

          - id: 3.1.4
            action: store
            key: "results.{test.id}"
            value:
              test: test
              response: response
              score: test_result.score
              passed: test_result.passed
              match_ratio: test_result.match_ratio

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 4: ANALYZE & REPORT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  - id: analyze
    name: "Analyze results"
    steps:
      - id: 4.1
        action: calculate
        data: results
        metrics:
          - total_tests: count
          - passed: "count(passed == true)"
          - failed: "count(passed == false)"
          - pass_rate: "passed / total_tests * 100"
          - total_points: "sum(score)"
          - max_points: "sum(test.points)"
        output: summary

      - id: 4.2
        action: categorize_failures
        data: results
        filter: "passed == false"
        output: failures

      - id: 4.3
        action: display
        template: test_report

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DISPLAY TEMPLATES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
display:
  test_report: |
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  REASONING TEST RESULTS                                        â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  Category: {test_category}                                     â•‘
    â•‘  Target: {target_agent || 'Direct test'}                       â•‘
    â•‘  Provider: {config.provider} ({config.model})                  â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  SUMMARY                                                        â•‘
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â•‘  Total Tests: {summary.total_tests}                            â•‘
    â•‘  Passed: {summary.passed} âœ…                                   â•‘
    â•‘  Failed: {summary.failed} âŒ                                   â•‘
    â•‘  Pass Rate: {summary.pass_rate:.1f}%                           â•‘
    â•‘  Points: {summary.total_points}/{summary.max_points}           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  test_details: |
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  TEST DETAILS                                                   â•‘
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    {for result in results.values()}
    â•‘  [{result.passed ? 'âœ…' : 'âŒ'}] {result.test.name}             â•‘
    â•‘      Prompt: {result.test.prompt[:50]}...                      â•‘
    â•‘      Expected: {result.test.keywords[:3]}                      â•‘
    â•‘      Match: {result.match_ratio:.0%}                           â•‘
    â•‘      Points: {result.score}/{result.test.points}               â•‘
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    {endfor}
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  failures_analysis: |
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  FAILURE ANALYSIS                                               â•‘
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    {for failure in failures}
    â•‘  âŒ {failure.test.name}                                        â•‘
    â•‘     Expected: {failure.test.keywords}                          â•‘
    â•‘     Got: {failure.response[:100]}...                           â•‘
    â•‘     Gap: {failure.analysis}                                    â•‘
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    {endfor}
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEST SCORING LOGIC
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
scoring:
  keyword_match:
    description: "Count keyword matches in response"
    algorithm: |
      matches = 0
      for keyword in expected_keywords:
          if keyword.lower() in response.lower():
              matches += 1
      ratio = matches / len(expected_keywords)
      passed = ratio >= threshold (default 0.6)
      score = passed ? test.points : 0

  partial_scoring:
    description: "Give partial credit for partial matches"
    algorithm: |
      ratio = matches / len(expected_keywords)
      score = test.points * ratio

  llm_grading:
    description: "Use LLM to grade response (more expensive)"
    prompt: |
      Grade this response on a scale of 0-{max_points}:

      Question: {test.prompt}
      Expected answer should contain: {test.keywords}
      Actual response: {response}

      Score (0-{max_points}):
