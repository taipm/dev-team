# LLM Agent System - Function Design Blueprint

> Generated by Blueprint Architect v3.0
> Date: 2026-01-04
> Type: Full System Design

---

## Metadata

| Field | Value |
|-------|-------|
| Project | LLM Agent System |
| Domain | AI Orchestration, Tool Execution, Memory Management |
| Functions | 12 |
| Complexity | Complex |
| Target Agent | go-dev-agent / python-dev-agent |

---

## SECTION 1: FUNCTION SPECIFICATIONS

### 1.1 AgentOrchestrator (Main)

#### processQuery

**Signature:**
```
processQuery(query: string, sessionId: string) → Result<AgentResponse, AgentError>
```

**Contract:**
- Pre: query is non-empty string
- Pre: sessionId is valid UUID
- Post(success): response contains answer + sources + tool_calls_made
- Post(failure): error ∈ {LLMError, ToolError, MemoryError, TimeoutError}
- Invariant: conversation history updated regardless of success/failure

**Complexity:** O(n) where n = number of tool calls needed

**Implementation Notes:**
- This is the main orchestrator - coordinates all other functions
- Implements ReAct loop (Reason → Act → Observe)
- Max iterations to prevent infinite loops

---

#### synthesizeResponse

**Signature:**
```
synthesizeResponse(toolResults: List<ToolResult>, originalQuery: string) → Result<string, SynthesisError>
```

**Contract:**
- Pre: toolResults is non-empty
- Pre: each ToolResult has status (success/error) and data
- Post(success): coherent natural language response
- Post(failure): error ∈ {LLMError, EmptyResultsError}

**Implementation Notes:**
- Uses LLM to combine multiple tool outputs
- Should reference sources appropriately

---

### 1.2 LLM Integration

#### callLLM

**Signature:**
```
callLLM(prompt: string, options: LLMOptions) → Result<LLMResponse, LLMError>
```

**Contract:**
- Pre: prompt.length <= MAX_PROMPT_LENGTH
- Pre: options.model is valid model identifier
- Post(success): response.content is non-empty
- Post(success): response.usage contains token counts
- Post(failure): error ∈ {RateLimitError, InvalidModelError, NetworkError, TimeoutError}
- Invariant: API key never logged

**Complexity:** O(1) but latency depends on model and prompt size

**Implementation Notes:**
- Implement retry with exponential backoff
- Track token usage for cost monitoring
- Support streaming for long responses

---

#### parseToolCalls

**Signature:**
```
parseToolCalls(llmResponse: LLMResponse) → List<ToolCall>
```

**Contract:**
- Pre: llmResponse is valid LLM output
- Post: returns list of ToolCall objects (may be empty)
- Post: each ToolCall has {tool_name, parameters, id}
- Invariant: malformed tool calls are skipped with warning

**Implementation Notes:**
- Handle both JSON tool_calls format and text parsing
- Validate tool names against registered tools
- Extract parameters as structured data

---

#### formatPrompt

**Signature:**
```
formatPrompt(query: string, history: List<Message>, tools: List<ToolDef>) → string
```

**Contract:**
- Pre: query is non-empty
- Post: prompt includes system instructions
- Post: prompt includes available tools (if any)
- Post: prompt includes relevant history
- Post: prompt.length <= context_window - expected_response_size
- Invariant: history is truncated if too long (summarize old turns)

**Implementation Notes:**
- Use template system for consistency
- Reserve tokens for response
- Include tool schemas in appropriate format

---

### 1.3 Tool Router

#### routeToTool

**Signature:**
```
routeToTool(toolCall: ToolCall, registry: ToolRegistry) → Result<Tool, RoutingError>
```

**Contract:**
- Pre: toolCall.tool_name is non-empty
- Pre: registry contains registered tools
- Post(success): returns matching Tool with execute method
- Post(failure): error ∈ {ToolNotFoundError, ToolDisabledError}

**Implementation Notes:**
- Support exact match and fuzzy matching
- Check tool permissions/enabled status
- Log routing decisions for debugging

---

#### executeTool

**Signature:**
```
executeTool(tool: Tool, params: ToolParams) → Result<ToolResult, ToolExecutionError>
```

**Contract:**
- Pre: tool is valid Tool object
- Pre: params match tool's parameter schema
- Post(success): result contains tool output
- Post(failure): error ∈ {ValidationError, ExecutionError, TimeoutError}
- Invariant: tool execution is sandboxed (no side effects on failure)

**Performance:**
- Timeout: configurable per tool (default 30s)
- Must handle long-running tools gracefully

**Concurrency:**
- Thread-safe: YES (tools run in isolation)
- Parallel execution: supported for independent tools

**Implementation Notes:**
- Validate params against tool schema before execution
- Capture stdout/stderr for debugging
- Implement timeout mechanism

---

#### validateToolResult

**Signature:**
```
validateToolResult(result: ToolResult, expectedSchema: Schema) → Result<ValidatedResult, ValidationError>
```

**Contract:**
- Pre: result is not null
- Post(success): result matches expected schema
- Post(failure): error contains specific validation failures

**Implementation Notes:**
- Use JSON Schema for validation
- Allow partial results with warnings
- Sanitize results before returning to LLM

---

### 1.4 Memory Manager

#### saveToMemory

**Signature:**
```
saveToMemory(sessionId: string, turn: ConversationTurn) → Result<void, MemoryError>
```

**Contract:**
- Pre: sessionId is valid
- Pre: turn contains {role, content, timestamp, tool_calls?}
- Post(success): turn persisted and retrievable
- Post(failure): error ∈ {StorageError, SerializationError}
- Invariant: memory is eventually consistent

**Implementation Notes:**
- Support multiple storage backends (Redis, PostgreSQL, in-memory)
- Include metadata for filtering/search
- Implement TTL for session expiry

---

#### loadContext

**Signature:**
```
loadContext(sessionId: string, limit: int) → Result<List<Message>, MemoryError>
```

**Contract:**
- Pre: sessionId is valid
- Pre: limit > 0 and limit <= MAX_CONTEXT_MESSAGES
- Post(success): returns recent messages in chronological order
- Post(not found): returns empty list (NOT error)

**Complexity:** O(1) for recent messages (indexed by session)

**Implementation Notes:**
- Return most recent N messages
- Include message metadata (timestamps, tool calls)
- Support filtering by role

---

#### summarizeHistory

**Signature:**
```
summarizeHistory(messages: List<Message>, targetTokens: int) → Result<string, SummarizationError>
```

**Contract:**
- Pre: messages is non-empty
- Pre: targetTokens > MIN_SUMMARY_TOKENS
- Post(success): summary.token_count <= targetTokens
- Post(success): summary preserves key information
- Post(failure): error ∈ {LLMError, EmptyInputError}

**Implementation Notes:**
- Use LLM for summarization
- Preserve important facts, decisions, tool results
- Can be done incrementally (summarize summaries)

---

## SECTION 2: DEPENDENCY GRAPH

```
                        processQuery (orchestrator)
                              │
         ┌────────────────────┼────────────────────┐
         │                    │                    │
         ▼                    ▼                    ▼
   formatPrompt          callLLM            synthesizeResponse
         │                    │                    │
         ▼                    ▼                    │
   loadContext          parseToolCalls            │
         │                    │                    │
         ▼                    ▼                    │
   summarizeHistory     routeToTool               │
                              │                    │
                              ▼                    │
                        executeTool ───────────────┘
                              │
                              ▼
                      validateToolResult
                              │
                              ▼
                        saveToMemory
```

**Implementation Order:**
1. **Leaf functions (no dependencies):**
   - validateToolResult
   - loadContext (basic version)

2. **Core utilities:**
   - callLLM (external API wrapper)
   - parseToolCalls
   - formatPrompt

3. **Tool system:**
   - routeToTool
   - executeTool

4. **Memory system:**
   - saveToMemory
   - summarizeHistory (uses callLLM)
   - loadContext (enhanced with summarization)

5. **Synthesis:**
   - synthesizeResponse

6. **Orchestrator (last):**
   - processQuery

---

## SECTION 3: ABSTRACTION LEAK WARNINGS

| Type | Abstract | Reality | Mitigation |
|------|----------|---------|------------|
| Latency | callLLM returns quickly | LLM can take 5-30s | Implement streaming, show progress |
| Token limits | Unlimited context | Context window limited (8K-128K) | Implement summarization, truncation |
| Tool reliability | Tools always work | External tools can fail | Retry logic, graceful degradation |
| Memory persistence | Memory always available | Storage can fail | Cache layer, fallback to stateless |
| Concurrency | Sequential execution | Parallel tools possible | Implement parallel execution safely |
| Cost | Free to call | Per-token pricing | Track usage, implement budgets |

---

## SECTION 4: OPEN QUESTIONS

- [ ] **LLM Provider:** Which provider? OpenAI, Anthropic, local?
- [ ] **Tool Schema Format:** OpenAI function calling format or custom?
- [ ] **Memory Backend:** Redis, PostgreSQL, or in-memory for MVP?
- [ ] **Streaming:** Should responses stream back to user?
- [ ] **Max Iterations:** How many ReAct loops before giving up? (Suggest: 5)
- [ ] **Parallel Tools:** Execute independent tools in parallel?
- [ ] **Error Recovery:** Retry failed tools or skip them?
- [ ] **Token Budget:** Max tokens per session/query?

---

## SECTION 5: CONCURRENCY CONTRACTS (Optional)

```yaml
Concurrency:
  processQuery:
    Thread Safety: ✓ Safe (each session isolated)
    Scalability: Horizontal (stateless with external memory)

  executeTool:
    Thread Safety: ✓ Safe (tools run in isolation)
    Parallel Execution: ✓ Supported for independent tools

  saveToMemory:
    Atomicity: Per-turn (each turn is atomic)
    Consistency: Eventually consistent

  callLLM:
    Thread Safety: ✓ Safe (HTTP client is thread-safe)
    Rate Limiting: Implement at caller level
```

---

## SECTION 6: ANNOTATED PSEUDOCODE (Optional)

```python
function processQuery(query, sessionId):
    // @pattern: ReAct (Reason-Act-Observe)
    // @complexity: O(iterations × tool_count)

    // Step 1: Load conversation context
    history = loadContext(sessionId, limit=20)

    // Step 2: Build initial prompt
    tools = getAvailableTools()
    prompt = formatPrompt(query, history, tools)

    // Step 3: ReAct loop
    max_iterations = 5
    tool_results = []

    for i in range(max_iterations):
        // Reason: Ask LLM what to do
        response = callLLM(prompt, options={tools: true})

        // Check if LLM wants to call tools
        tool_calls = parseToolCalls(response)

        if tool_calls.isEmpty():
            // LLM is done, synthesize final response
            break

        // Act: Execute each tool
        for call in tool_calls:
            tool = routeToTool(call, registry)
            result = executeTool(tool, call.params)
            validated = validateToolResult(result, tool.schema)
            tool_results.append(validated)

        // Observe: Add results to prompt for next iteration
        prompt = appendToolResults(prompt, tool_results)

    // Step 4: Synthesize final response
    if tool_results.isNotEmpty():
        answer = synthesizeResponse(tool_results, query)
    else:
        answer = response.content

    // Step 5: Save to memory
    saveToMemory(sessionId, {
        role: "user",
        content: query
    })
    saveToMemory(sessionId, {
        role: "assistant",
        content: answer,
        tool_calls: tool_results
    })

    return AgentResponse(answer, sources=tool_results)
```

---

## SECTION 7: DATA TYPES

```typescript
// Core Types
type SessionId = string  // UUID format

interface AgentResponse {
    answer: string
    sources: ToolResult[]
    tool_calls_made: int
    tokens_used: TokenUsage
}

interface ToolCall {
    id: string
    tool_name: string
    parameters: Record<string, any>
}

interface ToolResult {
    tool_call_id: string
    status: "success" | "error"
    data: any
    error?: string
}

interface ConversationTurn {
    role: "user" | "assistant" | "system" | "tool"
    content: string
    timestamp: DateTime
    tool_calls?: ToolCall[]
    tool_results?: ToolResult[]
}

interface LLMOptions {
    model: string
    temperature: float
    max_tokens: int
    tools?: ToolDef[]
}

interface ToolDef {
    name: string
    description: string
    parameters: JSONSchema
    enabled: boolean
}
```

---

## Handoff Summary

**Ready for:** go-dev-agent or python-dev-agent

**Functions to implement:** 12

**Suggested libraries:**

| Language | Area | Library |
|----------|------|---------|
| Go | HTTP Client | net/http, resty |
| Go | JSON | encoding/json |
| Go | Memory | go-redis/redis |
| Python | LLM | anthropic, openai |
| Python | Async | asyncio |
| Python | Memory | redis-py |

**Critical decisions made:**
1. ReAct pattern for agent loop
2. 4-layer abstraction (query → orchestrator → tools → memory)
3. Tool validation before and after execution
4. Session-based memory isolation

---

*Generated by Blueprint Architect v3.0*
*"From intent to implementation, elegantly."*
